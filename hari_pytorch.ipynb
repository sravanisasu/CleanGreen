{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hari_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtAjZ9q2E2h1oTl8xTpBam",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/CleanGreen/blob/master/hari_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjrxyQ587x7H"
      },
      "source": [
        "# General\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt \n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "# Scikit Learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(2)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUCPJInd721B"
      },
      "source": [
        "# Morlet Function\n",
        "def w_func(t):\n",
        "  return torch.cos(1.75*t)* torch.exp(-(t**2))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj3Q1vGx75Lg"
      },
      "source": [
        "batch_size= 32\n",
        "lr = 0.001\n",
        "mom = 0.999\n",
        "epochs=500\n",
        "filename = 'ObesityDataSet_raw_and_data_sinthetic.csv'\n",
        "data = pd.read_csv(filename)\n",
        "nin = len(data.columns)-1 # Size of input layer (sample size, number of features) \n",
        "nhn = 10 # Size of hidden layer\n",
        "non = 1"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqWcSAL7_yj"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = weight_norm(nn.Linear(nin, nhn, bias=False))\n",
        "        self.fc2 = weight_norm(nn.Linear(nhn, non, bias=False))\n",
        "        self.a = nn.Parameter(torch.rand(nhn), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.rand(nhn), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = (self.fc1(x)-self.b)/self.a\n",
        "        vk = self.fc2(w_func(t))\n",
        "        return vk\n",
        "\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54tB_cMi8Dig"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x,y):\n",
        "        self.data = []\n",
        "        for i,j in zip(x,y):\n",
        "            self.data.append((i,j))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @classmethod \n",
        "    def splits(cls, filename):\n",
        "        \n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "        data = pd.read_csv(filename)\n",
        "        #numeric_columns = data.select_dtypes(include=numerics)\n",
        "        cols_name = data.loc[:, data.dtypes == object].columns\n",
        "        data = data.replace(\"?\",\"NaN\")\n",
        "        data.fillna(data.mean())\n",
        "        for col_name in cols_name:\n",
        "            list_total = list(data[col_name])\n",
        "            list_uniq = list(set((list_total)))\n",
        "            data[col_name] = [list_uniq.index(data)+1 for data in list_total]\n",
        "        # Normalizing Data\n",
        "        data[data.columns] = minmax_scale(data[data.columns])\n",
        "        #data.to_csv(filename_p)\n",
        "        #print('\\n-------  DataFrame after normalization and encoding -------\\n')\n",
        "        #print(data_raw[0])\n",
        "        #print(data_raw.shape)\n",
        "        data_pre = np.array(data)\n",
        "        x=data_pre[:,:-1]\n",
        "        y=data_pre[:,-1]\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "        y_train=  torch.from_numpy(np.expand_dims(y_train, axis=1)).float()\n",
        "        y_test=  torch.from_numpy(np.expand_dims(y_test, axis=1)).float()\n",
        "        x_train =  torch.from_numpy(x_train).float()\n",
        "        x_test =  torch.from_numpy(x_test).float()\n",
        "        # print(x_train[0].shape,y_train[0].shape)\n",
        "        # print(x_train[1].shape,y_train[1].shape)\n",
        "        train = cls(x_train, y_train)\n",
        "        test = cls(x_test, y_test)\n",
        "        return train, test"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orntrKxB8HMo"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39mGo_zB8Hwq",
        "outputId": "0a7be196-99ce-47e8-f38a-76918b8fc4c1"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = Net()\n",
        "loss = 0\n",
        "losses = AverageMeter()\n",
        "total_loss = 0\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum=mom)\n",
        "train, test = CustomDataset.splits(filename)\n",
        "# t = torch.utils.data.Subset(train, [0])\n",
        "# print(t)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "loss_array = []\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    # print(\"epoch\")\n",
        "    # print(train[2].shape)\n",
        "    for x,y in train_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to('cuda')\n",
        "            y = y.to(\"cuda\")\n",
        "            model.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = nn.MSELoss()(pred, y)\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.update(loss.item())\n",
        "    loss_array.append(losses.val)\n",
        "    print_str = 'Epoch: [{0}]\\t' \\\n",
        "        'Loss= {loss.val:.6f} \\\n",
        "          Loss Avg: ({loss.avg:.6f})\\t' \\\n",
        "        .format(epoch, loss=losses) \n",
        "    print(print_str)\n",
        "\n",
        "end_time = time.time()\n",
        "print('\\n')\n",
        "print(end_time-start_time)\n",
        "# print(loss_array)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]\tLoss= 0.147487           Loss Avg: (0.194377)\t\n",
            "Epoch: [1]\tLoss= 0.122786           Loss Avg: (0.162897)\t\n",
            "Epoch: [2]\tLoss= 0.069455           Loss Avg: (0.140748)\t\n",
            "Epoch: [3]\tLoss= 0.092729           Loss Avg: (0.127422)\t\n",
            "Epoch: [4]\tLoss= 0.074723           Loss Avg: (0.119625)\t\n",
            "Epoch: [5]\tLoss= 0.064465           Loss Avg: (0.114826)\t\n",
            "Epoch: [6]\tLoss= 0.074452           Loss Avg: (0.110438)\t\n",
            "Epoch: [7]\tLoss= 0.081436           Loss Avg: (0.106709)\t\n",
            "Epoch: [8]\tLoss= 0.095197           Loss Avg: (0.103580)\t\n",
            "Epoch: [9]\tLoss= 0.065487           Loss Avg: (0.101024)\t\n",
            "Epoch: [10]\tLoss= 0.084348           Loss Avg: (0.098724)\t\n",
            "Epoch: [11]\tLoss= 0.073664           Loss Avg: (0.096650)\t\n",
            "Epoch: [12]\tLoss= 0.080341           Loss Avg: (0.094873)\t\n",
            "Epoch: [13]\tLoss= 0.080863           Loss Avg: (0.093197)\t\n",
            "Epoch: [14]\tLoss= 0.093252           Loss Avg: (0.091584)\t\n",
            "Epoch: [15]\tLoss= 0.081580           Loss Avg: (0.089989)\t\n",
            "Epoch: [16]\tLoss= 0.051908           Loss Avg: (0.088696)\t\n",
            "Epoch: [17]\tLoss= 0.066844           Loss Avg: (0.087451)\t\n",
            "Epoch: [18]\tLoss= 0.091736           Loss Avg: (0.086385)\t\n",
            "Epoch: [19]\tLoss= 0.079428           Loss Avg: (0.085352)\t\n",
            "Epoch: [20]\tLoss= 0.068354           Loss Avg: (0.084335)\t\n",
            "Epoch: [21]\tLoss= 0.070921           Loss Avg: (0.083414)\t\n",
            "Epoch: [22]\tLoss= 0.110017           Loss Avg: (0.082503)\t\n",
            "Epoch: [23]\tLoss= 0.057866           Loss Avg: (0.081587)\t\n",
            "Epoch: [24]\tLoss= 0.040860           Loss Avg: (0.080721)\t\n",
            "Epoch: [25]\tLoss= 0.062281           Loss Avg: (0.079849)\t\n",
            "Epoch: [26]\tLoss= 0.060714           Loss Avg: (0.079025)\t\n",
            "Epoch: [27]\tLoss= 0.030955           Loss Avg: (0.078178)\t\n",
            "Epoch: [28]\tLoss= 0.067741           Loss Avg: (0.077380)\t\n",
            "Epoch: [29]\tLoss= 0.054010           Loss Avg: (0.076580)\t\n",
            "Epoch: [30]\tLoss= 0.072051           Loss Avg: (0.075768)\t\n",
            "Epoch: [31]\tLoss= 0.037200           Loss Avg: (0.074960)\t\n",
            "Epoch: [32]\tLoss= 0.046944           Loss Avg: (0.074185)\t\n",
            "Epoch: [33]\tLoss= 0.028111           Loss Avg: (0.073459)\t\n",
            "Epoch: [34]\tLoss= 0.031527           Loss Avg: (0.072715)\t\n",
            "Epoch: [35]\tLoss= 0.034274           Loss Avg: (0.072005)\t\n",
            "Epoch: [36]\tLoss= 0.038766           Loss Avg: (0.071317)\t\n",
            "Epoch: [37]\tLoss= 0.046310           Loss Avg: (0.070578)\t\n",
            "Epoch: [38]\tLoss= 0.028040           Loss Avg: (0.069833)\t\n",
            "Epoch: [39]\tLoss= 0.017402           Loss Avg: (0.069076)\t\n",
            "Epoch: [40]\tLoss= 0.021788           Loss Avg: (0.068318)\t\n",
            "Epoch: [41]\tLoss= 0.047835           Loss Avg: (0.067570)\t\n",
            "Epoch: [42]\tLoss= 0.040966           Loss Avg: (0.066888)\t\n",
            "Epoch: [43]\tLoss= 0.034443           Loss Avg: (0.066225)\t\n",
            "Epoch: [44]\tLoss= 0.028668           Loss Avg: (0.065586)\t\n",
            "Epoch: [45]\tLoss= 0.031277           Loss Avg: (0.064957)\t\n",
            "Epoch: [46]\tLoss= 0.028624           Loss Avg: (0.064308)\t\n",
            "Epoch: [47]\tLoss= 0.051466           Loss Avg: (0.063644)\t\n",
            "Epoch: [48]\tLoss= 0.021819           Loss Avg: (0.062997)\t\n",
            "Epoch: [49]\tLoss= 0.034658           Loss Avg: (0.062363)\t\n",
            "Epoch: [50]\tLoss= 0.035517           Loss Avg: (0.061760)\t\n",
            "Epoch: [51]\tLoss= 0.021084           Loss Avg: (0.061179)\t\n",
            "Epoch: [52]\tLoss= 0.032719           Loss Avg: (0.060615)\t\n",
            "Epoch: [53]\tLoss= 0.033187           Loss Avg: (0.060048)\t\n",
            "Epoch: [54]\tLoss= 0.037002           Loss Avg: (0.059500)\t\n",
            "Epoch: [55]\tLoss= 0.037654           Loss Avg: (0.058955)\t\n",
            "Epoch: [56]\tLoss= 0.031257           Loss Avg: (0.058430)\t\n",
            "Epoch: [57]\tLoss= 0.023225           Loss Avg: (0.057902)\t\n",
            "Epoch: [58]\tLoss= 0.035774           Loss Avg: (0.057400)\t\n",
            "Epoch: [59]\tLoss= 0.022896           Loss Avg: (0.056907)\t\n",
            "Epoch: [60]\tLoss= 0.022741           Loss Avg: (0.056437)\t\n",
            "Epoch: [61]\tLoss= 0.026575           Loss Avg: (0.055985)\t\n",
            "Epoch: [62]\tLoss= 0.038671           Loss Avg: (0.055540)\t\n",
            "Epoch: [63]\tLoss= 0.028392           Loss Avg: (0.055105)\t\n",
            "Epoch: [64]\tLoss= 0.022606           Loss Avg: (0.054684)\t\n",
            "Epoch: [65]\tLoss= 0.023165           Loss Avg: (0.054274)\t\n",
            "Epoch: [66]\tLoss= 0.016290           Loss Avg: (0.053867)\t\n",
            "Epoch: [67]\tLoss= 0.033458           Loss Avg: (0.053477)\t\n",
            "Epoch: [68]\tLoss= 0.026228           Loss Avg: (0.053093)\t\n",
            "Epoch: [69]\tLoss= 0.020632           Loss Avg: (0.052715)\t\n",
            "Epoch: [70]\tLoss= 0.026465           Loss Avg: (0.052346)\t\n",
            "Epoch: [71]\tLoss= 0.029639           Loss Avg: (0.051990)\t\n",
            "Epoch: [72]\tLoss= 0.018086           Loss Avg: (0.051639)\t\n",
            "Epoch: [73]\tLoss= 0.036550           Loss Avg: (0.051294)\t\n",
            "Epoch: [74]\tLoss= 0.027059           Loss Avg: (0.050951)\t\n",
            "Epoch: [75]\tLoss= 0.020495           Loss Avg: (0.050617)\t\n",
            "Epoch: [76]\tLoss= 0.025924           Loss Avg: (0.050294)\t\n",
            "Epoch: [77]\tLoss= 0.031698           Loss Avg: (0.049972)\t\n",
            "Epoch: [78]\tLoss= 0.023698           Loss Avg: (0.049658)\t\n",
            "Epoch: [79]\tLoss= 0.016101           Loss Avg: (0.049351)\t\n",
            "Epoch: [80]\tLoss= 0.019665           Loss Avg: (0.049046)\t\n",
            "Epoch: [81]\tLoss= 0.016510           Loss Avg: (0.048746)\t\n",
            "Epoch: [82]\tLoss= 0.014527           Loss Avg: (0.048450)\t\n",
            "Epoch: [83]\tLoss= 0.032175           Loss Avg: (0.048163)\t\n",
            "Epoch: [84]\tLoss= 0.020232           Loss Avg: (0.047875)\t\n",
            "Epoch: [85]\tLoss= 0.021582           Loss Avg: (0.047598)\t\n",
            "Epoch: [86]\tLoss= 0.029467           Loss Avg: (0.047326)\t\n",
            "Epoch: [87]\tLoss= 0.042002           Loss Avg: (0.047058)\t\n",
            "Epoch: [88]\tLoss= 0.036290           Loss Avg: (0.046797)\t\n",
            "Epoch: [89]\tLoss= 0.018234           Loss Avg: (0.046536)\t\n",
            "Epoch: [90]\tLoss= 0.019779           Loss Avg: (0.046283)\t\n",
            "Epoch: [91]\tLoss= 0.014236           Loss Avg: (0.046028)\t\n",
            "Epoch: [92]\tLoss= 0.020270           Loss Avg: (0.045781)\t\n",
            "Epoch: [93]\tLoss= 0.017850           Loss Avg: (0.045537)\t\n",
            "Epoch: [94]\tLoss= 0.022231           Loss Avg: (0.045299)\t\n",
            "Epoch: [95]\tLoss= 0.029931           Loss Avg: (0.045061)\t\n",
            "Epoch: [96]\tLoss= 0.020417           Loss Avg: (0.044828)\t\n",
            "Epoch: [97]\tLoss= 0.021209           Loss Avg: (0.044599)\t\n",
            "Epoch: [98]\tLoss= 0.021838           Loss Avg: (0.044370)\t\n",
            "Epoch: [99]\tLoss= 0.032308           Loss Avg: (0.044147)\t\n",
            "Epoch: [100]\tLoss= 0.023178           Loss Avg: (0.043926)\t\n",
            "Epoch: [101]\tLoss= 0.009938           Loss Avg: (0.043708)\t\n",
            "Epoch: [102]\tLoss= 0.025241           Loss Avg: (0.043493)\t\n",
            "Epoch: [103]\tLoss= 0.031163           Loss Avg: (0.043281)\t\n",
            "Epoch: [104]\tLoss= 0.024369           Loss Avg: (0.043072)\t\n",
            "Epoch: [105]\tLoss= 0.039940           Loss Avg: (0.042870)\t\n",
            "Epoch: [106]\tLoss= 0.017457           Loss Avg: (0.042664)\t\n",
            "Epoch: [107]\tLoss= 0.022412           Loss Avg: (0.042464)\t\n",
            "Epoch: [108]\tLoss= 0.027958           Loss Avg: (0.042267)\t\n",
            "Epoch: [109]\tLoss= 0.019426           Loss Avg: (0.042069)\t\n",
            "Epoch: [110]\tLoss= 0.013995           Loss Avg: (0.041878)\t\n",
            "Epoch: [111]\tLoss= 0.016119           Loss Avg: (0.041687)\t\n",
            "Epoch: [112]\tLoss= 0.013598           Loss Avg: (0.041500)\t\n",
            "Epoch: [113]\tLoss= 0.014966           Loss Avg: (0.041315)\t\n",
            "Epoch: [114]\tLoss= 0.015206           Loss Avg: (0.041129)\t\n",
            "Epoch: [115]\tLoss= 0.019948           Loss Avg: (0.040949)\t\n",
            "Epoch: [116]\tLoss= 0.014373           Loss Avg: (0.040770)\t\n",
            "Epoch: [117]\tLoss= 0.013317           Loss Avg: (0.040592)\t\n",
            "Epoch: [118]\tLoss= 0.011892           Loss Avg: (0.040418)\t\n",
            "Epoch: [119]\tLoss= 0.024571           Loss Avg: (0.040243)\t\n",
            "Epoch: [120]\tLoss= 0.024938           Loss Avg: (0.040071)\t\n",
            "Epoch: [121]\tLoss= 0.026809           Loss Avg: (0.039903)\t\n",
            "Epoch: [122]\tLoss= 0.013330           Loss Avg: (0.039734)\t\n",
            "Epoch: [123]\tLoss= 0.019775           Loss Avg: (0.039570)\t\n",
            "Epoch: [124]\tLoss= 0.015959           Loss Avg: (0.039406)\t\n",
            "Epoch: [125]\tLoss= 0.012319           Loss Avg: (0.039244)\t\n",
            "Epoch: [126]\tLoss= 0.015714           Loss Avg: (0.039088)\t\n",
            "Epoch: [127]\tLoss= 0.038836           Loss Avg: (0.038932)\t\n",
            "Epoch: [128]\tLoss= 0.015398           Loss Avg: (0.038776)\t\n",
            "Epoch: [129]\tLoss= 0.018494           Loss Avg: (0.038620)\t\n",
            "Epoch: [130]\tLoss= 0.023401           Loss Avg: (0.038469)\t\n",
            "Epoch: [131]\tLoss= 0.011425           Loss Avg: (0.038317)\t\n",
            "Epoch: [132]\tLoss= 0.010009           Loss Avg: (0.038163)\t\n",
            "Epoch: [133]\tLoss= 0.007946           Loss Avg: (0.038010)\t\n",
            "Epoch: [134]\tLoss= 0.022910           Loss Avg: (0.037857)\t\n",
            "Epoch: [135]\tLoss= 0.018414           Loss Avg: (0.037706)\t\n",
            "Epoch: [136]\tLoss= 0.008684           Loss Avg: (0.037558)\t\n",
            "Epoch: [137]\tLoss= 0.016761           Loss Avg: (0.037411)\t\n",
            "Epoch: [138]\tLoss= 0.018710           Loss Avg: (0.037263)\t\n",
            "Epoch: [139]\tLoss= 0.019039           Loss Avg: (0.037116)\t\n",
            "Epoch: [140]\tLoss= 0.026032           Loss Avg: (0.036971)\t\n",
            "Epoch: [141]\tLoss= 0.025289           Loss Avg: (0.036826)\t\n",
            "Epoch: [142]\tLoss= 0.024244           Loss Avg: (0.036681)\t\n",
            "Epoch: [143]\tLoss= 0.013280           Loss Avg: (0.036538)\t\n",
            "Epoch: [144]\tLoss= 0.023254           Loss Avg: (0.036398)\t\n",
            "Epoch: [145]\tLoss= 0.015294           Loss Avg: (0.036258)\t\n",
            "Epoch: [146]\tLoss= 0.015063           Loss Avg: (0.036119)\t\n",
            "Epoch: [147]\tLoss= 0.012724           Loss Avg: (0.035981)\t\n",
            "Epoch: [148]\tLoss= 0.017443           Loss Avg: (0.035846)\t\n",
            "Epoch: [149]\tLoss= 0.023575           Loss Avg: (0.035712)\t\n",
            "Epoch: [150]\tLoss= 0.011458           Loss Avg: (0.035580)\t\n",
            "Epoch: [151]\tLoss= 0.018389           Loss Avg: (0.035448)\t\n",
            "Epoch: [152]\tLoss= 0.010605           Loss Avg: (0.035318)\t\n",
            "Epoch: [153]\tLoss= 0.025404           Loss Avg: (0.035190)\t\n",
            "Epoch: [154]\tLoss= 0.024382           Loss Avg: (0.035064)\t\n",
            "Epoch: [155]\tLoss= 0.019839           Loss Avg: (0.034939)\t\n",
            "Epoch: [156]\tLoss= 0.013082           Loss Avg: (0.034815)\t\n",
            "Epoch: [157]\tLoss= 0.027915           Loss Avg: (0.034691)\t\n",
            "Epoch: [158]\tLoss= 0.012085           Loss Avg: (0.034568)\t\n",
            "Epoch: [159]\tLoss= 0.021747           Loss Avg: (0.034444)\t\n",
            "Epoch: [160]\tLoss= 0.011962           Loss Avg: (0.034320)\t\n",
            "Epoch: [161]\tLoss= 0.021346           Loss Avg: (0.034198)\t\n",
            "Epoch: [162]\tLoss= 0.011018           Loss Avg: (0.034079)\t\n",
            "Epoch: [163]\tLoss= 0.015735           Loss Avg: (0.033961)\t\n",
            "Epoch: [164]\tLoss= 0.008263           Loss Avg: (0.033842)\t\n",
            "Epoch: [165]\tLoss= 0.012865           Loss Avg: (0.033726)\t\n",
            "Epoch: [166]\tLoss= 0.019498           Loss Avg: (0.033611)\t\n",
            "Epoch: [167]\tLoss= 0.015016           Loss Avg: (0.033495)\t\n",
            "Epoch: [168]\tLoss= 0.019004           Loss Avg: (0.033382)\t\n",
            "Epoch: [169]\tLoss= 0.021659           Loss Avg: (0.033270)\t\n",
            "Epoch: [170]\tLoss= 0.012657           Loss Avg: (0.033158)\t\n",
            "Epoch: [171]\tLoss= 0.014345           Loss Avg: (0.033048)\t\n",
            "Epoch: [172]\tLoss= 0.009308           Loss Avg: (0.032939)\t\n",
            "Epoch: [173]\tLoss= 0.022436           Loss Avg: (0.032829)\t\n",
            "Epoch: [174]\tLoss= 0.015882           Loss Avg: (0.032721)\t\n",
            "Epoch: [175]\tLoss= 0.009095           Loss Avg: (0.032613)\t\n",
            "Epoch: [176]\tLoss= 0.013476           Loss Avg: (0.032506)\t\n",
            "Epoch: [177]\tLoss= 0.024746           Loss Avg: (0.032400)\t\n",
            "Epoch: [178]\tLoss= 0.014498           Loss Avg: (0.032294)\t\n",
            "Epoch: [179]\tLoss= 0.014761           Loss Avg: (0.032190)\t\n",
            "Epoch: [180]\tLoss= 0.007614           Loss Avg: (0.032086)\t\n",
            "Epoch: [181]\tLoss= 0.011683           Loss Avg: (0.031985)\t\n",
            "Epoch: [182]\tLoss= 0.019112           Loss Avg: (0.031887)\t\n",
            "Epoch: [183]\tLoss= 0.026118           Loss Avg: (0.031793)\t\n",
            "Epoch: [184]\tLoss= 0.023055           Loss Avg: (0.031701)\t\n",
            "Epoch: [185]\tLoss= 0.008008           Loss Avg: (0.031608)\t\n",
            "Epoch: [186]\tLoss= 0.014066           Loss Avg: (0.031516)\t\n",
            "Epoch: [187]\tLoss= 0.010160           Loss Avg: (0.031423)\t\n",
            "Epoch: [188]\tLoss= 0.010012           Loss Avg: (0.031330)\t\n",
            "Epoch: [189]\tLoss= 0.012180           Loss Avg: (0.031239)\t\n",
            "Epoch: [190]\tLoss= 0.018958           Loss Avg: (0.031147)\t\n",
            "Epoch: [191]\tLoss= 0.006817           Loss Avg: (0.031055)\t\n",
            "Epoch: [192]\tLoss= 0.014048           Loss Avg: (0.030963)\t\n",
            "Epoch: [193]\tLoss= 0.009729           Loss Avg: (0.030873)\t\n",
            "Epoch: [194]\tLoss= 0.017890           Loss Avg: (0.030784)\t\n",
            "Epoch: [195]\tLoss= 0.010814           Loss Avg: (0.030697)\t\n",
            "Epoch: [196]\tLoss= 0.021943           Loss Avg: (0.030610)\t\n",
            "Epoch: [197]\tLoss= 0.022164           Loss Avg: (0.030521)\t\n",
            "Epoch: [198]\tLoss= 0.018023           Loss Avg: (0.030435)\t\n",
            "Epoch: [199]\tLoss= 0.015635           Loss Avg: (0.030348)\t\n",
            "Epoch: [200]\tLoss= 0.014033           Loss Avg: (0.030265)\t\n",
            "Epoch: [201]\tLoss= 0.012870           Loss Avg: (0.030181)\t\n",
            "Epoch: [202]\tLoss= 0.005994           Loss Avg: (0.030099)\t\n",
            "Epoch: [203]\tLoss= 0.011773           Loss Avg: (0.030017)\t\n",
            "Epoch: [204]\tLoss= 0.012164           Loss Avg: (0.029936)\t\n",
            "Epoch: [205]\tLoss= 0.008234           Loss Avg: (0.029853)\t\n",
            "Epoch: [206]\tLoss= 0.014994           Loss Avg: (0.029773)\t\n",
            "Epoch: [207]\tLoss= 0.028780           Loss Avg: (0.029693)\t\n",
            "Epoch: [208]\tLoss= 0.008792           Loss Avg: (0.029613)\t\n",
            "Epoch: [209]\tLoss= 0.012786           Loss Avg: (0.029532)\t\n",
            "Epoch: [210]\tLoss= 0.013940           Loss Avg: (0.029453)\t\n",
            "Epoch: [211]\tLoss= 0.007810           Loss Avg: (0.029375)\t\n",
            "Epoch: [212]\tLoss= 0.006694           Loss Avg: (0.029297)\t\n",
            "Epoch: [213]\tLoss= 0.016118           Loss Avg: (0.029221)\t\n",
            "Epoch: [214]\tLoss= 0.007734           Loss Avg: (0.029145)\t\n",
            "Epoch: [215]\tLoss= 0.015322           Loss Avg: (0.029069)\t\n",
            "Epoch: [216]\tLoss= 0.017425           Loss Avg: (0.028992)\t\n",
            "Epoch: [217]\tLoss= 0.012601           Loss Avg: (0.028916)\t\n",
            "Epoch: [218]\tLoss= 0.015896           Loss Avg: (0.028842)\t\n",
            "Epoch: [219]\tLoss= 0.009415           Loss Avg: (0.028768)\t\n",
            "Epoch: [220]\tLoss= 0.021029           Loss Avg: (0.028694)\t\n",
            "Epoch: [221]\tLoss= 0.009726           Loss Avg: (0.028621)\t\n",
            "Epoch: [222]\tLoss= 0.009529           Loss Avg: (0.028548)\t\n",
            "Epoch: [223]\tLoss= 0.011978           Loss Avg: (0.028476)\t\n",
            "Epoch: [224]\tLoss= 0.011145           Loss Avg: (0.028403)\t\n",
            "Epoch: [225]\tLoss= 0.011188           Loss Avg: (0.028332)\t\n",
            "Epoch: [226]\tLoss= 0.008820           Loss Avg: (0.028261)\t\n",
            "Epoch: [227]\tLoss= 0.010746           Loss Avg: (0.028191)\t\n",
            "Epoch: [228]\tLoss= 0.008901           Loss Avg: (0.028120)\t\n",
            "Epoch: [229]\tLoss= 0.015681           Loss Avg: (0.028050)\t\n",
            "Epoch: [230]\tLoss= 0.010170           Loss Avg: (0.027980)\t\n",
            "Epoch: [231]\tLoss= 0.008210           Loss Avg: (0.027912)\t\n",
            "Epoch: [232]\tLoss= 0.009680           Loss Avg: (0.027842)\t\n",
            "Epoch: [233]\tLoss= 0.011800           Loss Avg: (0.027773)\t\n",
            "Epoch: [234]\tLoss= 0.007503           Loss Avg: (0.027705)\t\n",
            "Epoch: [235]\tLoss= 0.014183           Loss Avg: (0.027637)\t\n",
            "Epoch: [236]\tLoss= 0.015954           Loss Avg: (0.027569)\t\n",
            "Epoch: [237]\tLoss= 0.016305           Loss Avg: (0.027503)\t\n",
            "Epoch: [238]\tLoss= 0.019064           Loss Avg: (0.027437)\t\n",
            "Epoch: [239]\tLoss= 0.011256           Loss Avg: (0.027371)\t\n",
            "Epoch: [240]\tLoss= 0.007552           Loss Avg: (0.027306)\t\n",
            "Epoch: [241]\tLoss= 0.012640           Loss Avg: (0.027242)\t\n",
            "Epoch: [242]\tLoss= 0.017154           Loss Avg: (0.027177)\t\n",
            "Epoch: [243]\tLoss= 0.011007           Loss Avg: (0.027114)\t\n",
            "Epoch: [244]\tLoss= 0.008893           Loss Avg: (0.027051)\t\n",
            "Epoch: [245]\tLoss= 0.017465           Loss Avg: (0.026989)\t\n",
            "Epoch: [246]\tLoss= 0.011562           Loss Avg: (0.026926)\t\n",
            "Epoch: [247]\tLoss= 0.012531           Loss Avg: (0.026865)\t\n",
            "Epoch: [248]\tLoss= 0.005637           Loss Avg: (0.026804)\t\n",
            "Epoch: [249]\tLoss= 0.023469           Loss Avg: (0.026743)\t\n",
            "Epoch: [250]\tLoss= 0.010874           Loss Avg: (0.026682)\t\n",
            "Epoch: [251]\tLoss= 0.012675           Loss Avg: (0.026622)\t\n",
            "Epoch: [252]\tLoss= 0.020038           Loss Avg: (0.026563)\t\n",
            "Epoch: [253]\tLoss= 0.012192           Loss Avg: (0.026503)\t\n",
            "Epoch: [254]\tLoss= 0.016317           Loss Avg: (0.026444)\t\n",
            "Epoch: [255]\tLoss= 0.011034           Loss Avg: (0.026386)\t\n",
            "Epoch: [256]\tLoss= 0.012040           Loss Avg: (0.026328)\t\n",
            "Epoch: [257]\tLoss= 0.012392           Loss Avg: (0.026271)\t\n",
            "Epoch: [258]\tLoss= 0.007797           Loss Avg: (0.026214)\t\n",
            "Epoch: [259]\tLoss= 0.019382           Loss Avg: (0.026157)\t\n",
            "Epoch: [260]\tLoss= 0.007852           Loss Avg: (0.026100)\t\n",
            "Epoch: [261]\tLoss= 0.005894           Loss Avg: (0.026045)\t\n",
            "Epoch: [262]\tLoss= 0.018445           Loss Avg: (0.025989)\t\n",
            "Epoch: [263]\tLoss= 0.011933           Loss Avg: (0.025933)\t\n",
            "Epoch: [264]\tLoss= 0.011863           Loss Avg: (0.025878)\t\n",
            "Epoch: [265]\tLoss= 0.008815           Loss Avg: (0.025823)\t\n",
            "Epoch: [266]\tLoss= 0.014771           Loss Avg: (0.025769)\t\n",
            "Epoch: [267]\tLoss= 0.012063           Loss Avg: (0.025715)\t\n",
            "Epoch: [268]\tLoss= 0.018091           Loss Avg: (0.025661)\t\n",
            "Epoch: [269]\tLoss= 0.015187           Loss Avg: (0.025608)\t\n",
            "Epoch: [270]\tLoss= 0.008613           Loss Avg: (0.025556)\t\n",
            "Epoch: [271]\tLoss= 0.008269           Loss Avg: (0.025503)\t\n",
            "Epoch: [272]\tLoss= 0.010461           Loss Avg: (0.025452)\t\n",
            "Epoch: [273]\tLoss= 0.011822           Loss Avg: (0.025400)\t\n",
            "Epoch: [274]\tLoss= 0.015202           Loss Avg: (0.025350)\t\n",
            "Epoch: [275]\tLoss= 0.008348           Loss Avg: (0.025299)\t\n",
            "Epoch: [276]\tLoss= 0.005719           Loss Avg: (0.025249)\t\n",
            "Epoch: [277]\tLoss= 0.009399           Loss Avg: (0.025202)\t\n",
            "Epoch: [278]\tLoss= 0.023376           Loss Avg: (0.025154)\t\n",
            "Epoch: [279]\tLoss= 0.012611           Loss Avg: (0.025105)\t\n",
            "Epoch: [280]\tLoss= 0.005515           Loss Avg: (0.025057)\t\n",
            "Epoch: [281]\tLoss= 0.012355           Loss Avg: (0.025010)\t\n",
            "Epoch: [282]\tLoss= 0.014674           Loss Avg: (0.024963)\t\n",
            "Epoch: [283]\tLoss= 0.013102           Loss Avg: (0.024915)\t\n",
            "Epoch: [284]\tLoss= 0.009211           Loss Avg: (0.024867)\t\n",
            "Epoch: [285]\tLoss= 0.019079           Loss Avg: (0.024821)\t\n",
            "Epoch: [286]\tLoss= 0.010600           Loss Avg: (0.024774)\t\n",
            "Epoch: [287]\tLoss= 0.012892           Loss Avg: (0.024728)\t\n",
            "Epoch: [288]\tLoss= 0.010273           Loss Avg: (0.024681)\t\n",
            "Epoch: [289]\tLoss= 0.009220           Loss Avg: (0.024635)\t\n",
            "Epoch: [290]\tLoss= 0.019858           Loss Avg: (0.024590)\t\n",
            "Epoch: [291]\tLoss= 0.012912           Loss Avg: (0.024545)\t\n",
            "Epoch: [292]\tLoss= 0.014531           Loss Avg: (0.024501)\t\n",
            "Epoch: [293]\tLoss= 0.011716           Loss Avg: (0.024457)\t\n",
            "Epoch: [294]\tLoss= 0.014687           Loss Avg: (0.024415)\t\n",
            "Epoch: [295]\tLoss= 0.011977           Loss Avg: (0.024371)\t\n",
            "Epoch: [296]\tLoss= 0.004858           Loss Avg: (0.024328)\t\n",
            "Epoch: [297]\tLoss= 0.008252           Loss Avg: (0.024285)\t\n",
            "Epoch: [298]\tLoss= 0.013269           Loss Avg: (0.024242)\t\n",
            "Epoch: [299]\tLoss= 0.017576           Loss Avg: (0.024199)\t\n",
            "Epoch: [300]\tLoss= 0.016101           Loss Avg: (0.024157)\t\n",
            "Epoch: [301]\tLoss= 0.013975           Loss Avg: (0.024115)\t\n",
            "Epoch: [302]\tLoss= 0.018144           Loss Avg: (0.024073)\t\n",
            "Epoch: [303]\tLoss= 0.005367           Loss Avg: (0.024030)\t\n",
            "Epoch: [304]\tLoss= 0.010588           Loss Avg: (0.023989)\t\n",
            "Epoch: [305]\tLoss= 0.010607           Loss Avg: (0.023947)\t\n",
            "Epoch: [306]\tLoss= 0.014922           Loss Avg: (0.023906)\t\n",
            "Epoch: [307]\tLoss= 0.005189           Loss Avg: (0.023864)\t\n",
            "Epoch: [308]\tLoss= 0.022367           Loss Avg: (0.023823)\t\n",
            "Epoch: [309]\tLoss= 0.005986           Loss Avg: (0.023782)\t\n",
            "Epoch: [310]\tLoss= 0.014064           Loss Avg: (0.023741)\t\n",
            "Epoch: [311]\tLoss= 0.016060           Loss Avg: (0.023702)\t\n",
            "Epoch: [312]\tLoss= 0.017529           Loss Avg: (0.023662)\t\n",
            "Epoch: [313]\tLoss= 0.015497           Loss Avg: (0.023623)\t\n",
            "Epoch: [314]\tLoss= 0.010874           Loss Avg: (0.023584)\t\n",
            "Epoch: [315]\tLoss= 0.009344           Loss Avg: (0.023545)\t\n",
            "Epoch: [316]\tLoss= 0.012288           Loss Avg: (0.023507)\t\n",
            "Epoch: [317]\tLoss= 0.005976           Loss Avg: (0.023468)\t\n",
            "Epoch: [318]\tLoss= 0.015162           Loss Avg: (0.023429)\t\n",
            "Epoch: [319]\tLoss= 0.014533           Loss Avg: (0.023391)\t\n",
            "Epoch: [320]\tLoss= 0.014282           Loss Avg: (0.023353)\t\n",
            "Epoch: [321]\tLoss= 0.005256           Loss Avg: (0.023314)\t\n",
            "Epoch: [322]\tLoss= 0.009238           Loss Avg: (0.023276)\t\n",
            "Epoch: [323]\tLoss= 0.013681           Loss Avg: (0.023237)\t\n",
            "Epoch: [324]\tLoss= 0.010959           Loss Avg: (0.023200)\t\n",
            "Epoch: [325]\tLoss= 0.017653           Loss Avg: (0.023163)\t\n",
            "Epoch: [326]\tLoss= 0.009478           Loss Avg: (0.023125)\t\n",
            "Epoch: [327]\tLoss= 0.007300           Loss Avg: (0.023089)\t\n",
            "Epoch: [328]\tLoss= 0.007945           Loss Avg: (0.023051)\t\n",
            "Epoch: [329]\tLoss= 0.012389           Loss Avg: (0.023013)\t\n",
            "Epoch: [330]\tLoss= 0.008291           Loss Avg: (0.022976)\t\n",
            "Epoch: [331]\tLoss= 0.012642           Loss Avg: (0.022940)\t\n",
            "Epoch: [332]\tLoss= 0.008159           Loss Avg: (0.022904)\t\n",
            "Epoch: [333]\tLoss= 0.011515           Loss Avg: (0.022868)\t\n",
            "Epoch: [334]\tLoss= 0.007311           Loss Avg: (0.022833)\t\n",
            "Epoch: [335]\tLoss= 0.032118           Loss Avg: (0.022799)\t\n",
            "Epoch: [336]\tLoss= 0.011825           Loss Avg: (0.022763)\t\n",
            "Epoch: [337]\tLoss= 0.011156           Loss Avg: (0.022728)\t\n",
            "Epoch: [338]\tLoss= 0.013830           Loss Avg: (0.022693)\t\n",
            "Epoch: [339]\tLoss= 0.015683           Loss Avg: (0.022659)\t\n",
            "Epoch: [340]\tLoss= 0.005982           Loss Avg: (0.022625)\t\n",
            "Epoch: [341]\tLoss= 0.009473           Loss Avg: (0.022592)\t\n",
            "Epoch: [342]\tLoss= 0.011701           Loss Avg: (0.022559)\t\n",
            "Epoch: [343]\tLoss= 0.009262           Loss Avg: (0.022525)\t\n",
            "Epoch: [344]\tLoss= 0.008431           Loss Avg: (0.022492)\t\n",
            "Epoch: [345]\tLoss= 0.006711           Loss Avg: (0.022460)\t\n",
            "Epoch: [346]\tLoss= 0.012844           Loss Avg: (0.022428)\t\n",
            "Epoch: [347]\tLoss= 0.009456           Loss Avg: (0.022395)\t\n",
            "Epoch: [348]\tLoss= 0.016436           Loss Avg: (0.022361)\t\n",
            "Epoch: [349]\tLoss= 0.014585           Loss Avg: (0.022329)\t\n",
            "Epoch: [350]\tLoss= 0.011652           Loss Avg: (0.022296)\t\n",
            "Epoch: [351]\tLoss= 0.006501           Loss Avg: (0.022265)\t\n",
            "Epoch: [352]\tLoss= 0.018003           Loss Avg: (0.022233)\t\n",
            "Epoch: [353]\tLoss= 0.017293           Loss Avg: (0.022201)\t\n",
            "Epoch: [354]\tLoss= 0.005432           Loss Avg: (0.022168)\t\n",
            "Epoch: [355]\tLoss= 0.013292           Loss Avg: (0.022136)\t\n",
            "Epoch: [356]\tLoss= 0.009762           Loss Avg: (0.022104)\t\n",
            "Epoch: [357]\tLoss= 0.005208           Loss Avg: (0.022072)\t\n",
            "Epoch: [358]\tLoss= 0.006524           Loss Avg: (0.022041)\t\n",
            "Epoch: [359]\tLoss= 0.017761           Loss Avg: (0.022009)\t\n",
            "Epoch: [360]\tLoss= 0.012525           Loss Avg: (0.021979)\t\n",
            "Epoch: [361]\tLoss= 0.007640           Loss Avg: (0.021948)\t\n",
            "Epoch: [362]\tLoss= 0.013544           Loss Avg: (0.021917)\t\n",
            "Epoch: [363]\tLoss= 0.010921           Loss Avg: (0.021886)\t\n",
            "Epoch: [364]\tLoss= 0.016941           Loss Avg: (0.021856)\t\n",
            "Epoch: [365]\tLoss= 0.010562           Loss Avg: (0.021827)\t\n",
            "Epoch: [366]\tLoss= 0.009483           Loss Avg: (0.021797)\t\n",
            "Epoch: [367]\tLoss= 0.010781           Loss Avg: (0.021767)\t\n",
            "Epoch: [368]\tLoss= 0.007384           Loss Avg: (0.021737)\t\n",
            "Epoch: [369]\tLoss= 0.011540           Loss Avg: (0.021707)\t\n",
            "Epoch: [370]\tLoss= 0.003645           Loss Avg: (0.021678)\t\n",
            "Epoch: [371]\tLoss= 0.007722           Loss Avg: (0.021649)\t\n",
            "Epoch: [372]\tLoss= 0.009184           Loss Avg: (0.021619)\t\n",
            "Epoch: [373]\tLoss= 0.012258           Loss Avg: (0.021590)\t\n",
            "Epoch: [374]\tLoss= 0.016184           Loss Avg: (0.021560)\t\n",
            "Epoch: [375]\tLoss= 0.012137           Loss Avg: (0.021531)\t\n",
            "Epoch: [376]\tLoss= 0.007150           Loss Avg: (0.021502)\t\n",
            "Epoch: [377]\tLoss= 0.016805           Loss Avg: (0.021473)\t\n",
            "Epoch: [378]\tLoss= 0.006938           Loss Avg: (0.021444)\t\n",
            "Epoch: [379]\tLoss= 0.008621           Loss Avg: (0.021416)\t\n",
            "Epoch: [380]\tLoss= 0.008545           Loss Avg: (0.021387)\t\n",
            "Epoch: [381]\tLoss= 0.016210           Loss Avg: (0.021358)\t\n",
            "Epoch: [382]\tLoss= 0.005150           Loss Avg: (0.021329)\t\n",
            "Epoch: [383]\tLoss= 0.005261           Loss Avg: (0.021301)\t\n",
            "Epoch: [384]\tLoss= 0.007634           Loss Avg: (0.021273)\t\n",
            "Epoch: [385]\tLoss= 0.015621           Loss Avg: (0.021245)\t\n",
            "Epoch: [386]\tLoss= 0.011737           Loss Avg: (0.021218)\t\n",
            "Epoch: [387]\tLoss= 0.014265           Loss Avg: (0.021191)\t\n",
            "Epoch: [388]\tLoss= 0.009194           Loss Avg: (0.021164)\t\n",
            "Epoch: [389]\tLoss= 0.025863           Loss Avg: (0.021137)\t\n",
            "Epoch: [390]\tLoss= 0.014944           Loss Avg: (0.021110)\t\n",
            "Epoch: [391]\tLoss= 0.007143           Loss Avg: (0.021083)\t\n",
            "Epoch: [392]\tLoss= 0.013754           Loss Avg: (0.021056)\t\n",
            "Epoch: [393]\tLoss= 0.010456           Loss Avg: (0.021029)\t\n",
            "Epoch: [394]\tLoss= 0.007550           Loss Avg: (0.021003)\t\n",
            "Epoch: [395]\tLoss= 0.008739           Loss Avg: (0.020975)\t\n",
            "Epoch: [396]\tLoss= 0.011749           Loss Avg: (0.020950)\t\n",
            "Epoch: [397]\tLoss= 0.009552           Loss Avg: (0.020925)\t\n",
            "Epoch: [398]\tLoss= 0.017376           Loss Avg: (0.020901)\t\n",
            "Epoch: [399]\tLoss= 0.009015           Loss Avg: (0.020876)\t\n",
            "Epoch: [400]\tLoss= 0.016030           Loss Avg: (0.020850)\t\n",
            "Epoch: [401]\tLoss= 0.013613           Loss Avg: (0.020827)\t\n",
            "Epoch: [402]\tLoss= 0.021875           Loss Avg: (0.020802)\t\n",
            "Epoch: [403]\tLoss= 0.007386           Loss Avg: (0.020776)\t\n",
            "Epoch: [404]\tLoss= 0.012099           Loss Avg: (0.020751)\t\n",
            "Epoch: [405]\tLoss= 0.012851           Loss Avg: (0.020727)\t\n",
            "Epoch: [406]\tLoss= 0.016862           Loss Avg: (0.020701)\t\n",
            "Epoch: [407]\tLoss= 0.015562           Loss Avg: (0.020676)\t\n",
            "Epoch: [408]\tLoss= 0.007751           Loss Avg: (0.020651)\t\n",
            "Epoch: [409]\tLoss= 0.012567           Loss Avg: (0.020626)\t\n",
            "Epoch: [410]\tLoss= 0.016434           Loss Avg: (0.020601)\t\n",
            "Epoch: [411]\tLoss= 0.020359           Loss Avg: (0.020576)\t\n",
            "Epoch: [412]\tLoss= 0.017271           Loss Avg: (0.020552)\t\n",
            "Epoch: [413]\tLoss= 0.006624           Loss Avg: (0.020527)\t\n",
            "Epoch: [414]\tLoss= 0.009021           Loss Avg: (0.020503)\t\n",
            "Epoch: [415]\tLoss= 0.014909           Loss Avg: (0.020478)\t\n",
            "Epoch: [416]\tLoss= 0.004971           Loss Avg: (0.020453)\t\n",
            "Epoch: [417]\tLoss= 0.006226           Loss Avg: (0.020429)\t\n",
            "Epoch: [418]\tLoss= 0.009844           Loss Avg: (0.020404)\t\n",
            "Epoch: [419]\tLoss= 0.007716           Loss Avg: (0.020380)\t\n",
            "Epoch: [420]\tLoss= 0.010902           Loss Avg: (0.020357)\t\n",
            "Epoch: [421]\tLoss= 0.008088           Loss Avg: (0.020333)\t\n",
            "Epoch: [422]\tLoss= 0.006272           Loss Avg: (0.020311)\t\n",
            "Epoch: [423]\tLoss= 0.007761           Loss Avg: (0.020288)\t\n",
            "Epoch: [424]\tLoss= 0.014188           Loss Avg: (0.020265)\t\n",
            "Epoch: [425]\tLoss= 0.007718           Loss Avg: (0.020242)\t\n",
            "Epoch: [426]\tLoss= 0.012254           Loss Avg: (0.020220)\t\n",
            "Epoch: [427]\tLoss= 0.011927           Loss Avg: (0.020197)\t\n",
            "Epoch: [428]\tLoss= 0.009802           Loss Avg: (0.020175)\t\n",
            "Epoch: [429]\tLoss= 0.020780           Loss Avg: (0.020153)\t\n",
            "Epoch: [430]\tLoss= 0.009198           Loss Avg: (0.020132)\t\n",
            "Epoch: [431]\tLoss= 0.014338           Loss Avg: (0.020109)\t\n",
            "Epoch: [432]\tLoss= 0.016890           Loss Avg: (0.020087)\t\n",
            "Epoch: [433]\tLoss= 0.017835           Loss Avg: (0.020065)\t\n",
            "Epoch: [434]\tLoss= 0.015765           Loss Avg: (0.020043)\t\n",
            "Epoch: [435]\tLoss= 0.005850           Loss Avg: (0.020021)\t\n",
            "Epoch: [436]\tLoss= 0.013877           Loss Avg: (0.019998)\t\n",
            "Epoch: [437]\tLoss= 0.019472           Loss Avg: (0.019977)\t\n",
            "Epoch: [438]\tLoss= 0.016402           Loss Avg: (0.019955)\t\n",
            "Epoch: [439]\tLoss= 0.006263           Loss Avg: (0.019933)\t\n",
            "Epoch: [440]\tLoss= 0.008652           Loss Avg: (0.019912)\t\n",
            "Epoch: [441]\tLoss= 0.013902           Loss Avg: (0.019890)\t\n",
            "Epoch: [442]\tLoss= 0.010167           Loss Avg: (0.019868)\t\n",
            "Epoch: [443]\tLoss= 0.009600           Loss Avg: (0.019846)\t\n",
            "Epoch: [444]\tLoss= 0.005427           Loss Avg: (0.019824)\t\n",
            "Epoch: [445]\tLoss= 0.007943           Loss Avg: (0.019802)\t\n",
            "Epoch: [446]\tLoss= 0.018159           Loss Avg: (0.019780)\t\n",
            "Epoch: [447]\tLoss= 0.004813           Loss Avg: (0.019758)\t\n",
            "Epoch: [448]\tLoss= 0.008493           Loss Avg: (0.019737)\t\n",
            "Epoch: [449]\tLoss= 0.014102           Loss Avg: (0.019715)\t\n",
            "Epoch: [450]\tLoss= 0.005169           Loss Avg: (0.019694)\t\n",
            "Epoch: [451]\tLoss= 0.014010           Loss Avg: (0.019672)\t\n",
            "Epoch: [452]\tLoss= 0.011288           Loss Avg: (0.019651)\t\n",
            "Epoch: [453]\tLoss= 0.012697           Loss Avg: (0.019630)\t\n",
            "Epoch: [454]\tLoss= 0.009944           Loss Avg: (0.019608)\t\n",
            "Epoch: [455]\tLoss= 0.008502           Loss Avg: (0.019587)\t\n",
            "Epoch: [456]\tLoss= 0.010473           Loss Avg: (0.019566)\t\n",
            "Epoch: [457]\tLoss= 0.004505           Loss Avg: (0.019545)\t\n",
            "Epoch: [458]\tLoss= 0.010240           Loss Avg: (0.019524)\t\n",
            "Epoch: [459]\tLoss= 0.007122           Loss Avg: (0.019504)\t\n",
            "Epoch: [460]\tLoss= 0.006325           Loss Avg: (0.019483)\t\n",
            "Epoch: [461]\tLoss= 0.003046           Loss Avg: (0.019462)\t\n",
            "Epoch: [462]\tLoss= 0.006205           Loss Avg: (0.019442)\t\n",
            "Epoch: [463]\tLoss= 0.009180           Loss Avg: (0.019422)\t\n",
            "Epoch: [464]\tLoss= 0.011267           Loss Avg: (0.019402)\t\n",
            "Epoch: [465]\tLoss= 0.009215           Loss Avg: (0.019382)\t\n",
            "Epoch: [466]\tLoss= 0.011171           Loss Avg: (0.019362)\t\n",
            "Epoch: [467]\tLoss= 0.012095           Loss Avg: (0.019343)\t\n",
            "Epoch: [468]\tLoss= 0.008037           Loss Avg: (0.019323)\t\n",
            "Epoch: [469]\tLoss= 0.005986           Loss Avg: (0.019303)\t\n",
            "Epoch: [470]\tLoss= 0.009926           Loss Avg: (0.019284)\t\n",
            "Epoch: [471]\tLoss= 0.006088           Loss Avg: (0.019264)\t\n",
            "Epoch: [472]\tLoss= 0.013428           Loss Avg: (0.019244)\t\n",
            "Epoch: [473]\tLoss= 0.012817           Loss Avg: (0.019225)\t\n",
            "Epoch: [474]\tLoss= 0.007530           Loss Avg: (0.019206)\t\n",
            "Epoch: [475]\tLoss= 0.008361           Loss Avg: (0.019187)\t\n",
            "Epoch: [476]\tLoss= 0.012158           Loss Avg: (0.019168)\t\n",
            "Epoch: [477]\tLoss= 0.011554           Loss Avg: (0.019150)\t\n",
            "Epoch: [478]\tLoss= 0.017098           Loss Avg: (0.019131)\t\n",
            "Epoch: [479]\tLoss= 0.010166           Loss Avg: (0.019113)\t\n",
            "Epoch: [480]\tLoss= 0.012054           Loss Avg: (0.019095)\t\n",
            "Epoch: [481]\tLoss= 0.012817           Loss Avg: (0.019077)\t\n",
            "Epoch: [482]\tLoss= 0.011216           Loss Avg: (0.019059)\t\n",
            "Epoch: [483]\tLoss= 0.007745           Loss Avg: (0.019042)\t\n",
            "Epoch: [484]\tLoss= 0.011353           Loss Avg: (0.019024)\t\n",
            "Epoch: [485]\tLoss= 0.012033           Loss Avg: (0.019007)\t\n",
            "Epoch: [486]\tLoss= 0.007095           Loss Avg: (0.018990)\t\n",
            "Epoch: [487]\tLoss= 0.004322           Loss Avg: (0.018973)\t\n",
            "Epoch: [488]\tLoss= 0.004651           Loss Avg: (0.018957)\t\n",
            "Epoch: [489]\tLoss= 0.009536           Loss Avg: (0.018941)\t\n",
            "Epoch: [490]\tLoss= 0.008118           Loss Avg: (0.018924)\t\n",
            "Epoch: [491]\tLoss= 0.013730           Loss Avg: (0.018908)\t\n",
            "Epoch: [492]\tLoss= 0.008115           Loss Avg: (0.018891)\t\n",
            "Epoch: [493]\tLoss= 0.009264           Loss Avg: (0.018874)\t\n",
            "Epoch: [494]\tLoss= 0.007483           Loss Avg: (0.018857)\t\n",
            "Epoch: [495]\tLoss= 0.010889           Loss Avg: (0.018840)\t\n",
            "Epoch: [496]\tLoss= 0.015441           Loss Avg: (0.018823)\t\n",
            "Epoch: [497]\tLoss= 0.011762           Loss Avg: (0.018806)\t\n",
            "Epoch: [498]\tLoss= 0.007187           Loss Avg: (0.018789)\t\n",
            "Epoch: [499]\tLoss= 0.021593           Loss Avg: (0.018773)\t\n",
            "\n",
            "\n",
            "150.4753930568695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6Qh8Oai8NKp",
        "outputId": "454e148f-69f3-4ff5-b42a-f0dca66e9008"
      },
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "losses = AverageMeter()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for x,y in test_loader:\n",
        "    # print(x.tolist(), y.shape)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.to('cuda')\n",
        "        y = y.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "      pred = model(x)\n",
        "      test_loss += nn.MSELoss()(pred, y)\n",
        "      losses.update(loss.item())\n",
        "      y = y.cpu()\n",
        "      pred = pred.cpu()\n",
        "      for y_d in y:\n",
        "        y_true.append(y_d.item())\n",
        "      for pred_d in pred:\n",
        "        y_pred.append(pred_d.item())\n",
        "print_str = 'Loss: {loss.val:.6f}\\\n",
        "             Loss Avg: ({loss.avg:.6f})\\t' \\\n",
        "             .format(loss=losses) \n",
        "print(print_str)\n",
        "# y_true = np"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.021593             Loss Avg: (0.021593)\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP_-Kzjv8NsJ",
        "outputId": "c1244747-4963-4199-fe3d-e3f3b9a6ea40"
      },
      "source": [
        "r2_score(y_true, y_pred)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8746242099733068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8GMGBlK8P_0",
        "outputId": "b1d6f2fe-b674-4c9f-c2da-9c1ccc7fc863"
      },
      "source": [
        "mean_absolute_error(y_true, y_pred)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0836277668734233"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "SB0QhP_e8TmM",
        "outputId": "684b1c39-d7c3-42f3-b6e4-943fad4a4f5d"
      },
      "source": [
        "plt.plot(loss_array)\n",
        "plt.xlabel('Epochs'); \n",
        "plt.ylabel('Error');"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}